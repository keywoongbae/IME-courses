# 8. Finite-Horizon MDP

## Optimality Criteria

### 설명

- policy를 결정하게 되면, 이에 해당하는 reward sequence를 random으로 반환받음.
- 이 reward sequence가 최대한 커지도록 하는 policy를 선택하는 문제.
- random 시퀀스들 가운데 최대 reward를 선택.

### Notations

- $h_{t+1} = (h_t, a_t, s_{t+1})$이며, $h_t = (s_1, a_1, s_2, a_2, .. ,s_t)$
- $X_t$는 t시점에서 state에 대한 확률 변수
- $Y_t$는 t시점에서 action에 대한 확률 변수
- 보상변수 R은 $\begin{cases} R_t  = g_t(X_t, Y_t)& \mbox{if } t < N\\ R_N  = g_N(s_N)& \mbox{if }t=N\end{cases}$

### 정책의 성격에 따라 달라지는 보상함수

- If $\pi  = (d_1, .., d_N) \in \Pi^{HR}$ (Historically Dependent and Random)
  - $R = (g_1(X_1, Y_1), .. g_{N-1}(X_{N-1}, Y_{N-1}), g_N(X_N))$ (확실하지 않음)
- If $\pi = (d_1, ..., d_N) \in \Pi^{HD}$ (Historically Dependent and Deterministic)
  - $R = (g_1(X_1, d_1(h_1)), .. g_{N-1}(X_{N-1}, d_{N-1}(h_{N-1})), g_N(X_N))$
- If $\pi \in \Pi^{MD}$ (Markovian and Deterministic)
  - $R = (g_1(X_1, d_1(X_1)), .., g_{N-1}(X_{N-1}, d_{N-1}(X_{N-1})), g_N(s_N))$

### Expected Total Reward Criterion

- Optimality Criteria는 모든 v에 대해서 $\varphi(u) \geq \varphi(v)$를 만족하는 reward sequence $u = (u_1 ,..., u_N)$을 찾는 일이며, 이는 사람마다 선택하게 되는 효용함수가 다름.
- 두 정책 $\pi, \pi'$ 중에 $E^{\pi}[\varphi(R)] \geq E^{\pi'}[\varphi(R)]$를 만족하게 되면 $\pi'$보다 $\pi$를 더 선호한다 할 수 있음.
- 이때 $E^{\pi}[\varphi(R)]$는 $\pi$라는 정책 하에서의 기대 효용값이며, $\Sigma_r \varphi(r)P^{\pi}(R=r)$로 계산함.
- 이때, 임이의 기대효용값은 계산이 어려우므로, $\varphi$가 인자들에 대한 선형함수라고 가정하며 다음과 같이 설정함.
  - $\varphi(g_1, ..., g_N) = \Sigma g_t$

### 정책의 성격에 따라 달라지는 기대효용값

- If $\pi \in \Pi^{HR}$
  - $v^{\pi}_N(s) \equiv E^{\pi}_s [\Sigma_{t=1}^{N-1}g_t(X_t, Y_t) + g_N(X_N)]$
- If $\pi \in \Pi^{HD}$
  - $v^{\pi}_N(s) \equiv E^{\pi}_s [\Sigma_{t=1}^{N-1}g_t(X_t, d_t(h_t)) + g_N(X_N)]$

### Discount Factor

- 시간적 요소를 고려하기 위해 discount factor를 도입함.
- $t+1$ 시점에 받은 단위 보상 1개에 대한 $t$시점의 값을 측정하는 실수 $\lambda$가 discount factor.
- finite-horizon에서는 적용하나 안하나 큰 차이는 없으나, infinite-horizon에서는 차이가 매우 큼.
- If $\pi \in \Pi^{HR}$
  - $v^{\pi}_{N,\lambda}(s) \equiv E^{\pi}_s [\Sigma_{t=1}^{N-1} \lambda ^{t-1}g_t(X_t, Y_t) + \lambda ^{N-1} g_N(X_N)]$

### Tolerance

- 만약 $s \in S, \pi \in \Pi^k$일 때 다음과 같은 조건을 만족시키면 $\pi^*$는 최적의 정책이라고 할 수 있음.
  - $v^{\pi^*}_N(s) \geq v^{\pi}_N(s)$, k = {MD, MR, HD, HR}
- 만약 $s \in S, \pi \in \Pi^k$이고, tolerance값인 $\epsilon>0$에 대해서  $\pi^*_{\epsilon}$는 $\epsilon-$optimal policy라고 할 수 있음.
  - $v^{\pi^*_{\epsilon}}_N(s) \geq v^{\pi}_N(s) - \epsilon$, k = {MD, MR, HD, HR}