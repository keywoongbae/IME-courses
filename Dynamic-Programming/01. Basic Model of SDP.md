## Introduction to Dynamic Programming

### Sequential Decision Process

- 정의
  - 종종 불확실성에 직면할 때 어떤 목표를 향해 취해지는 일련의 행동을 수반하는 활동.
  - ![image-20230915141024849](.\img\image-20230915141024849.png)
  - a set of {action epochs, system states, available actions, state and action dependent immediate rewards or costs, state and action dependent transition probabilities.}

### Dynamic Programming

- 정의
  - 귀납적 계산에 기초한 순차적 의사결정 과정을 해결하기 위한 수학적 도구.

### Basic Model of SDP

- 
- Let T denote the set of decision epochs. This subset of the non-negative real numbers may be classified in two ways : as either a discrete set or a continuous and as either a finite and infinite set.
- Let's focus on the discrete and finite set case in the basic model, T = {1,2,.. , N}. (This sequential decision process is called a finite horizon problem.)
- Then, there exists a finite number of stages(or periods), a decision epoch corresponds to the beginning of a stage, and decisions are made at decision epochs
- Let $s_t$ be the state of the system at decision epoch $t$, $s_t \in S$ and $t \in T$, where $S$ is the set of possible states
- Let $a_t$ be the decision (or action) to be selected at a decision epoch t, 
- $S$와 $A_s$는 유한한 집합이다.
- $t$시점에서의 상태 $s_t$에서는 $a_t$라는 액션이 발생하는데, 이것의 결과는 보상 아니면 비용으로 나타나며 $g(s_t, a_t)$로 표현된다.
- 기본 모델의 표현
  - <img src=".\img\image-20230915114807167.png" alt="image-20230915114807167" style="zoom: 80%;" />
- 관찰
  - $t$시점에서의 상태를 $s_t$라고 할 때, 그 다음 단계 $s_{t+1}$는 $s_t$와 $a_t$로 결정된다.
    - $s_{t+1} = f_t(s_t, a_t)$
    - $f_t$는 dynamic system, system equation이라고도 불리며, state가 업데이트 될 때 시스템 혹은 메커니즘을 묘사함.
- Problem Objective
  - 초기 상태인 $s_0$가 주어졌을 때, $T$시점에서 전체 cost를 최소화하기 위해 일렬의 액션을 선택하는 것.
- A decision rule specifies the action to be chosen at a particular decision epoch.
  - This decision rule can be deterministic or randomized
  - this decision rule can be markovian, $a_t = d_t(s_t)$, or history dependent, $a_t = d_t (h_t), where h_t = (s_1, a_1, .., s_{t-1}, a_{t-1}, s_t)$ is a history.
  - in this basic model, we will focus on deterministic markovian decision rule case.
- A policy specifies the sequence of decision rule under any possible future state or history.$\pi = (d_1,d_2, .. ,d_{N-1})$
  - When $d_t(s_t) \in A_s$, then $\pi$ is an admissible policy
- A optimal policy is a policy which minimizes the total cost.

### Principle of Optimality

- 정의
  - decision epoch인 $n$에서의 state인 $s_n$에 대해서, $s_n$을 현재 상태라고 했을 때 $n$에서 $N$까지 최적의 cost-to-go 함수(Value Function)는 다음과 같음.
  - $V^*_n(s_n) = \underset{a_n, a_{n+1}, ..., a_{N-1} \in A_s}{\min} [g_N(s_N) + \Sigma_{t-n}^{N-1} g_t(S-t, a_t)]$
  - <img src=".\img\image-20230915114807167.png" alt="image-20230915114807167" style="zoom: 80%;" />
- 정의
  - Optimality의 원리는 아래 statement를 따름.
  - $\pi^* = (d^*_1, d^*_2,.., d^*_{N-1})$가 basic problem에서 최적의 policy라고 가정함.
  - 이때 $n \geq 1$에 대해서, truncated policy $(d^*_n, d^*_{n+1},.., d^*_{N-1})$도 $N-n$단계의 tail subproblem에 최적화되어있음. 함수는 다음과 같음.
  - $V^*_n(s_n) = [g_N(s_N) + \Sigma^{N-1}_{t=n} g_t(s_t, d^*(s_t))]$
  - boundary condition을 적용하면 $V^*_N(s_N) = g_N(s_N)$ 
- Example : Maximizing Utilities
  - $U(a)$ : 특정 개인이 $a$라는 양의 금액을 사용했을 때 얻는 만족감을 나타내는 효용 함수.
  - $U(0) = 0$이며, $U'(a) > 0, \ \forall a >0$
  - $t$ 시점에서 개인이 가지고 있는 자본의 수준을 $x_t$라고 함.
  - $t$ 시점에서 개인이 사용하는 금액은 $a_t \ (0 \leq a_t \leq x_t)$이며,  남은 금액은 $t+1$ 시점에서 투자된다. 그 결과 $t+1$시점에서 자본은 $x_{t+1} = \beta(x_t - a_t), \ \beta >1$
  - $x_T$는 효용에 어떠한 영향도 줄 수 없다고 가정함.
  - 전체 효용은 $U(a_1) +U(a_2) +..U(a_{T-1}) $.
  - 목적은 시간이 지나면서 전체 효용을 최대화하는 것.
  - 이때 decision variable(action)은?
    - $f_{T-n}(x)$는 $T-n$시점에서 자본이 $x$라고 할 때, 남은 $n$개의 단계동안 얻게 될 최대 효용 값.
    - $U(a)=a$일 때, $f_{T-n}(x)=\beta^{n-1}x, \ \forall n \geq 1, \forall x \geq 0$임을 보여라.
      - Will be continued





